{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] The specified procedure could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=1.12s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:32<00:00, 12768.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.55s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import os\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "from utils import clean_sentences, calculate_bleu_scores\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('./cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "\n",
    "\n",
    "batch_size = 64                # batch size\n",
    "vocab_threshold = 5            # minimum word count threshold\n",
    "vocab_from_file = True         # if True, load existing vocab file\n",
    "embed_size = 300               # dimensionality of image and word embeddings\n",
    "hidden_size = 256              # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 1                 # number of training epochs\n",
    "train_images_folder = \"train2014\"\n",
    "val_images_folder = \"val2014\"\n",
    "train_annotations_file = \"captions_train2014.json\"\n",
    "val_annotations_file = \"captions_val2014.json\"\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),                          \n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      \n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "\n",
    "# Build data loader.\n",
    "train_loader = get_loader(transform=train_transform,\n",
    "                          mode='train',\n",
    "                          batch_size=batch_size,\n",
    "                          vocab_threshold=vocab_threshold,\n",
    "                          vocab_from_file=vocab_from_file,\n",
    "                          img_folder=train_images_folder,\n",
    "                          annotations_file=train_annotations_file)\n",
    "val_loader = get_loader(transform=val_transform,\n",
    "                        mode='test',\n",
    "                        batch_size=batch_size,\n",
    "                        img_folder=val_images_folder,\n",
    "                        annotations_file=val_annotations_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(train_loader.dataset.vocab)        # type: ignore\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "params = list(decoder.parameters()) + list(encoder.parameters())\n",
    "\n",
    "optimizer = optim.Adam(params, lr=3e-4)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_steps = math.ceil(len(train_loader.dataset.caption_lengths) / train_loader.batch_sampler.batch_size) # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader, total_steps):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i_step in tqdm(range(1, total_steps+1)):\n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)     # type: ignore\n",
    "        loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        preds = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(preds.view(-1, vocab_size), captions.view(-1))\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "        break\n",
    "\n",
    "    return epoch_loss / total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(loader):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    results = []\n",
    "\n",
    "    for images, img_ids in tqdm(loader):\n",
    "        with torch.no_grad():\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Obtain the embedded image features.\n",
    "            features = encoder(images).unsqueeze(1)\n",
    "\n",
    "            # Pass the embedded image features through the model to get a predicted caption.\n",
    "            output = decoder.generate_captions(features)\n",
    "            sentences = clean_sentences(loader.dataset.vocab.idx2word, output)\n",
    "            results.extend([{\"image_id\": img_id.item(), \"caption\": sentence} for img_id, sentence in zip(img_ids, sentences)])  # type: ignore \n",
    "        break\n",
    "    with open(\"results.json\", 'w') as res_file:\n",
    "        json.dump(results, res_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d909e6d43674e818216727edbdd8f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2103f049d751490499cd33f4132422ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/633 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(train_loader, total_steps)\n",
    "    validate(val_loader)\n",
    "    bleu_scores = calculate_bleu_scores(val_loader.dataset.coco, \"results.json\")    # type: ignore\n",
    " \n",
    "    with open(\"statistics.txt\", 'a') as file:\n",
    "        file.write(str(train_loss) + '\\n')\n",
    "        file.write(' '.join(map(str, bleu_scores)) + '\\n\\n')\n",
    "        \n",
    "    torch.save(encoder.state_dict(), \"./models/encoder.pt\")\n",
    "    torch.save(decoder.state_dict(), \"./models/decoder.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
